{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will Bring the Dataset in the form tag: {Tokenized Sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DATA PREPROCESSING CELL  #\n",
    "#===========================#\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "# Path to Initial Dataset\n",
    "path = './quotes.json'\n",
    "\n",
    "# Import JSON as Pandas Dataframe\n",
    "df = pd.read_json(path, orient='columns')\n",
    "# df.head(5)\n",
    "\n",
    "# Remove list for all useless Tags\n",
    "remove_list = ['attributed-no-source']\n",
    "\n",
    "# Function to Remove Redundant Columns and Tags \n",
    "def curate_data(data):    \n",
    "    data = data.drop(['Author', 'Popularity', 'Category'], axis = 1)\n",
    "    data.drop_duplicates(subset =\"Quote\", keep = 'first', inplace = True) \n",
    "    data['Tags'] = data['Tags'].apply(lambda tags_list: [x for x in tags_list if x not in remove_list and not x.startswith('misattributed')])        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to Apply Text Preprocessing\n",
    "def preprocess(sentence):\n",
    "    return [_ for _ in word_tokenize(sentence.lower().translate(str.maketrans('', '', string.punctuation))) if _ not in stop]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Redundant Data\n",
    "curated_data = curate_data(df)\n",
    "curated_data.index = range(len(curated_data))\n",
    "\n",
    "# Write Curated Data to JSON\n",
    "curated_data.to_json(r'./curated-data.json')\n",
    "\n",
    "# Test with the first 10 Quotes to Implement Preprocessing\n",
    "testdf = curated_data[:10]\n",
    "testdf.Quote = testdf.Quote.apply(preprocess)\n",
    "testdf = testdf.explode('Tags').reset_index(drop=True)\n",
    "testdf.head(50)\n",
    "\n",
    "\n",
    "# testdf['Quote'].apply(lambda x: x.lower()).apply(lambda x: nltk.word_tokenize(x)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cry', 'crying', 'experience', 'happiness', 'joy', 'life', 'optimism', 'sadness', 'smile', 'smiling ', 'best', 'love', 'mistakes', 'out-of-control', 'truth', 'worst ', 'be-yourself', 'honesty', 'inspirational', 'human-nature', 'humor', 'infinity', 'philosophy', 'science', 'stupidity', 'universe ', 'ataraxy', 'confidence', 'fitting-in', 'individuality', 'those-who-matter ', 'dance', 'heaven', 'hurt', 'sing ', 'dreams', 'reality', 'sleep ', 'books', 'simile', 'soul ', 'humor ', 'life ']\n"
     ]
    }
   ],
   "source": [
    "# Identify Unique Tags\n",
    "# Create flat list of Tags\n",
    "tags = testdf['Tags'].tolist()\n",
    "flat_tags = list(dict.fromkeys(tags))\n",
    "print(flat_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>cry</th>\n",
       "      <th>crying</th>\n",
       "      <th>experience</th>\n",
       "      <th>happiness</th>\n",
       "      <th>joy</th>\n",
       "      <th>life</th>\n",
       "      <th>optimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>smile</th>\n",
       "      <th>smiling</th>\n",
       "      <th>...</th>\n",
       "      <th>hurt</th>\n",
       "      <th>sing</th>\n",
       "      <th>dreams</th>\n",
       "      <th>reality</th>\n",
       "      <th>sleep</th>\n",
       "      <th>books</th>\n",
       "      <th>simile</th>\n",
       "      <th>soul</th>\n",
       "      <th>humor</th>\n",
       "      <th>life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(cry,), (crying,), (experience,), (happiness,), (joy,), (life,), (optimism,), (sadness,), (smile,), (smiling ,), (best,), (love,), (mistakes,), (out-of-control,), (truth,), (worst ,), (be-yourself,), (honesty,), (inspirational,), (human-nature,), (humor,), (infinity,), (philosophy,), (science,), (stupidity,), (universe ,), (ataraxy,), (confidence,), (fitting-in,), (individuality,), (those-who-matter ,), (dance,), (heaven,), (hurt,), (sing ,), (dreams,), (reality,), (sleep ,), (books,), (simile,), (soul ,), (humor ,), (life ,)]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 43 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create New Dataframe with Unique Tags as Columns\n",
    "testdf3 = testdf.sort_values(by=['Tags'])\n",
    "testdf4 = pd.DataFrame(columns=[flat_tags])\n",
    "testdf4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shorttext\n",
    "#  TRAIN DATA LOADING CELL  #\n",
    "#===========================#\n",
    "\n",
    "# Subject Dataset\n",
    "trainclassdict = shorttext.data.subjectkeywords()\n",
    "print(trainclassdict)\n",
    "\n",
    "\n",
    "# NIH RePORT Dataset\n",
    "#trainclassdict = shorttext.data.nihreports()\n",
    "#print(trainclassdict)\n",
    "\n",
    "# Inaugural Addresses Dataset\n",
    "#trainclassdict = shorttext.data.inaugural()\n",
    "\n",
    "\n",
    "# User Defined Dataset (JSON)\n",
    "#trainclassdict = shorttext.data.retrieve_jsondata_as_dict('./quotes.json')\n",
    "#print(trainclassdict[0])\n",
    "# User Defined Dataset (CSV Heading & at least 2 columns)\n",
    "# trainclassdict = shorttext.data.retrieve_csvdata_as_dict('/path/to/file.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
